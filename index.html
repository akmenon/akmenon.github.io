<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>

  <link rel="stylesheet" type="text/css" href="style.css"/>
  <script src="scripts.js"></script>

  <meta charset="utf-8">
  <meta name="description" content="Aditya Krishna Menon - Research Scientist, Google">
  <meta name="author" content="Aditya Menon">  

  <title>Homepage of Aditya Krishna Menon</title>

  <meta name="viewport" content="width=device-width, initial-scale=1">

</head>

<body>

  <header id="header">
    <div class="wrapper header">
      <h1>Aditya Krishna Menon</h1>
      <nav>
          <ul>
              <li><a href="index.html#bio_anchor">Bio</a></li>
              <li><a href="index.html#interests_anchor">Research</a></li>
              <li><a href="publications.html">Publications</a></li>
              <li><a href="talks.html">Talks</a></li>
              <li><a href="misc.html">Misc</a></li>
          </ul>
      </nav>
    </div>
  </header>

  <div id="content">

    <div style="clear: left;">
      <img src="pics/itsme_2018.png" style="display:inline-block; vertical-align: top; width: 75%; max-width: 207px;" alt="What I look like"/>
      &nbsp;
      <span style="display:inline-block; text-align: justify" align="right">
          Research Scientist
          <p>Google</p>
          <p>New York, NY</p>
          <p>Email: <tt>adityakmenon</tt> followed by <tt>google.com</tt></p>
      </span>
    </div>

    <!-- -->
    <h2 class="divider" id="bio_anchor">Biography</h2>

    <div id="bio_div">
    I'm a research scientist at Google.
    I work on machine learning and its applications.
    <br><br>

    I completed my honours in Computer Science from the University of Sydney in 2006 under <a href=https://www.hbku.edu.qa/en/staff/sanjay-chawla/>Sanjay Chawla</a>.
    I completed my PhD in Computer Science from UC San Diego in 2013 under <a href=http://cseweb.ucsd.edu/~elkan/>Charles Elkan</a>.
    From 2013 - 2018,
    I held positions at NICTA, CSIRO Data61,
    and the Australian National University,
    working with <a href="https://fm.ls/bob/">Bob Williamson</a> and <a href="http://www.ong-home.my/">other</a> <a href="http://users.cecs.anu.edu.au/~rnock/">inimitable</a> <a href="https://www.sutd.edu.sg/profile/young-lee">colleagues</a>.

    <br><br>

    Here is a copy of my <a href="resume/resume_full.pdf">CV</a>.
    </div>

    <br>

    <!-- -->
    <h2 class="divider" id="interests_anchor">Research interests</h2>

    <div id="interests_div">
    I am broadly interested in the design and analysis of machine learning algorithms for (weakly-) supervised learning problems occurring in practice.
    Current areas of interest include:
    <ul>
      <li>Efficient inference for large (language) models (e.g., knowledge distillation, model cascading)</li>
      <li>Retrieval and re-ranking (e.g., negative mining, loss function design)</li>
      <li>Foundations of (weakly-)supervised learning (e.g. class-probability estimation, bipartite ranking)</li>
      <!-- <li>Weakly-supervised learning (e.g. learning from label noise, positive and unlabelled learning)</li> -->
      <!-- <li>Classification with real-world constraints (e.g. class imbalance, fairness)</li> -->
      <!-- <li>Relations amongst foundational problems (e.g. class-probability estimation, bipartite ranking)</li> -->
    </ul>
    I have also worked on (and remain interested in) long-tail learning, learning from label noise, and collaborative filtering.

    <!-- <br> -->
    <h3 class="divider">Selected publications</h3>

      <div id="publications_div" style="padding-left: 1em">
        Below are a few representative publications.
        For a full list, see <a href="publications">here</a>.
        <br><br>

        Inspired by <a href="http://www.cs.princeton.edu/~chazelle/linernotes.html">Bernard Chazelle's</a> wonderful idea of "liner notes" for his papers, I've included some of my own for a few papers.
        (Liner notes are not peer-, or even coauthor-reviewed.)

        <ul style="padding: 1em; list-style: none;">
          <li> <strong>Post-hoc estimators for learning to defer to an expert</strong>.
          <br>
          Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya Krishna Menon, Ankit Singh Rawat, and Sanjiv Kumar.
          <br>
          In <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022.
          <br>
          [<a href="https://openreview.net/pdf?id=_jg6Sf6tuF7">pdf</a>]
          <br><br>

          <li> <strong>A statistical perspective on distillation</strong>.
          <br>
          Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, and Sanjiv Kumar.
          <br>
          In <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2021.
          <br>
          [<a href="papers/distillation/Bayes_Distillation.pdf">pdf</a>]
          <br><br>

          <li> <strong>Long-tail learning via logit adjustment</strong>.
          <br>
          Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar.
          <br>
          In <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2021.
          <br>
          [<a href="https://openreview.net/pdf?id=37nvvqkCo5">pdf</a>]
          <br><br>

  <!-- 
          <li> <strong>Can gradient clipping mitigate label noise?</strong>
          <br>
          Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar.
          <br>
          In <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, Addis Ababa 2020.
          <br>
          [<a href="https://openreview.net/pdf?id=rklB76EKPr">pdf</a>]
          <br><br>
   -->

          <li> <strong>Does label smoothing mitigate label noise?</strong>
          <br>
          Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar.
          <br>
          In <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2020.
          <br>
          [<a href="https://arxiv.org/pdf/2003.02819">pdf</a>]
          <br><br> 

<!--           <li> <strong>Fairness risk measures</strong>.
          <br>
          Robert C. Williamson and Aditya Krishna Menon.
          <br>
          In <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, Long Beach, 2019.
          <br>
          [<a href="https://arxiv.org/pdf/1901.08665">pdf</a>]
          <br><br>
 -->
 
  <!--
          <li> <strong>Learning from binary labels with instance-dependent corruption</strong>.
          <br>
          Aditya Krishna Menon, Brendan van Rooyen, and Nagarajan Natarajan.<br>    
          In <em><strong>Machine Learning</strong></em>, 2018.<br>
          [<a href="https://arxiv.org/pdf/1605.00751">pdf</a>]
          <br><br> 
  -->

  <!--
          <li> <strong>Bipartite ranking: a risk-theoretic perspective</strong>.
          <br>
          Aditya Krishna Menon and Robert C. Williamson.<br>
          In <em>Journal of Machine Learning Research (<strong>JMLR</strong>)</em>, Volume 17, Issue 195. 2016.<br>
          [<a href="http://jmlr.org/papers/volume17/14-265/14-265.pdf">pdf</a>] [<a href="javascript:;" onclick="showDiv('liner_bipartite');">liner notes</a>]  

            <div id="liner_bipartite" style="display:none">
              <br>
              <p style='color:gray'>
              Back in 2011, months of exhaustive experimentation with AUC maximisation in the context of link prediction taught me one thing: it's hard to do better than logistic regression. I was excited when later that year, <a href=http://www.icml-2011.org/papers/567_icmlpaper.pdf>Kot≈Çowski et al.</a> gave a compelling theoretical explanation for why this might be.
              <br><br>
              When I joined NICTA in 2013, I followed a rite of passage and read Mark Reid and Bob Williamson's <a href=http://www.jmlr.org/papers/v12/reid11a.html>tour de force</a> on information, divergences, and risks. Tucked away in that paper was a mention of how the AUC related to the concepts presented. I innocently mentioned to Bob that perhaps there was more to be said here. He agreed, and suggested that I start by spelling out what was in my mind.
              <br><br>
              We never expected it would take quite that long to spell.
              [<a href="javascript:;" onclick="hideDiv('liner_bipartite');">hide</a>]
              </p>
            </div>
          <br><br>
   -->

          <li> <strong>Linking losses for density ratio and class-probability estimation.</strong>
          <br>
          Aditya Krishna Menon and Cheng Soon Ong.<br>
          In <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, New York City, 2016.<br>
          [<a href="papers/density-ratio/density-ratio-paper.pdf">pdf</a>] [<a href="papers/density-ratio/density-ratio-slides.pdf">slides</a>] [<a href="papers/density-ratio/density-ratio-poster.pdf">poster</a>] [<a href="papers/density-ratio/index.html">code</a>] [<a href="javascript:;" onclick="showDiv('liner_density');">liner notes</a>]  

            <div id="liner_density" style="display:none">
              <br>
              <p style='color:gray'>
              Long before being introduced to proper losses, I'd known about, and completely accepted, the basic premise as to why density ratio was different to class-probability estimation: the later only <em>indirectly</em> modelled the ratio, which is generally a sub-optimal strategy. Sometime in late 2015, I re-read the cool work on <a href=http://www.jmlr.org/papers/volume10/kanamori09a/kanamori09a.pdf>LSIF</a>. This time, with link functions and partial losses fresh in my mind, I noticed that one could think of this as involving a particular proper loss plus link function. This was mildly interesting, but the point remained: using anything but the link that directly yields the density ratio must be a bad idea.
              <br><br>
              This was something of a defeat for my running theory at the time, that most everything can be attacked with some version of logistic regression. To better understand this failure mode, I was looking at Sugiyama's <a href=http://www.ism.ac.jp/editsec/aism/pdf/10463_2011_Article_343.pdf>elegant unified Bregman view</a> of density ratios. I noticed that logistic regression was mentioned, but didn't pay much attention to it; after all, the Bregman view of regret under proper losses immediately led to the KL minimisation view of logistic regression.
              <br><br>
              It was only on a re-read that I realised there was a bit more to this innocuous result: it was a statement of Bregman minimisation not to the true probability, but to the true density ratio. Now that I didn't know was true for logistic regression. Studying the surprisingly simple proof, I saw it could be viewed as a surprising equality between the KL divergence on probabilities and on ratios.
              <br><br>
              I was sure this couldn't be true in general; but why? I worked through the case of a general divergence, trying to find out the line where the proof would break.
              <br><br>
              Half an hour later, after staring at my working multiple times, I was still incredulous that everything seemed to work in general, too. And so was Lemma 2 born. [<a href="javascript:;" onclick="hideDiv('liner_density');">hide</a>]            
              </p>            
            </div>
          <br><br>
        </ul>
      </div>
    </div>

    <br>

  </div>

</body>

</html>
